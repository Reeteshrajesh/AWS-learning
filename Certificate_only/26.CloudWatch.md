## Great! Let's dive into the **first topic**:

## üü£ **1. Introduction to Amazon CloudWatch**

---

### ‚úÖ **What is Amazon CloudWatch?**

**Amazon CloudWatch** is a fully managed **observability and monitoring** service for AWS resources, on-premise infrastructure, and applications. It collects and visualizes **metrics**, **logs**, and **events** in near real-time, enabling you to monitor system health, detect issues automatically, and take actions based on predefined rules.

> Think of it as AWS‚Äôs ‚Äúmission control‚Äù to keep watch over every part of your infrastructure and applications.

---

### üîÑ **Key Differences Between CloudWatch, CloudTrail, X-Ray, and Config**

| Service        | Purpose    | Key Focus                                              |
| -------------- | ---------- | ------------------------------------------------------ |
| **CloudWatch** | Monitoring | Metrics, logs, alarms, dashboards, automation          |
| **CloudTrail** | Auditing   | Records API calls/events for governance and compliance |
| **AWS X-Ray**  | Tracing    | Analyzes request paths in distributed applications     |
| **AWS Config** | Compliance | Tracks configuration changes and compliance rules      |

#### ‚úÖ Summary:

* **CloudWatch** helps detect **performance and operational issues**.
* **CloudTrail** helps trace **who did what and when** (auditing).
* **X-Ray** helps trace **request journeys** across microservices.
* **Config** helps you enforce **infrastructure compliance**.

---

### üåü **Core Benefits of CloudWatch**

#### üîπ **1. Observability**

* Collects **metrics** from AWS services (e.g., CPU, memory, latency)
* Aggregates and searches **application logs**
* Visualizes data with **dashboards and insights**

#### üîπ **2. Automation**

* Trigger **Alarms** based on metric thresholds
* Invoke **Lambda functions** automatically
* Use **EventBridge (formerly CW Events)** to schedule tasks or respond to state changes

#### üîπ **3. Optimization**

* Identify **underutilized** resources (e.g., EC2, RDS)
* Analyze performance bottlenecks
* Improve system **resilience and cost efficiency** through alerting and actions

---

### üõ† Example Use Cases (Production Perspective)

| Use Case                               | CloudWatch Role                               |
| -------------------------------------- | --------------------------------------------- |
| Restart EC2 if it fails health check   | Create alarm + auto recovery action           |
| Alert on high memory usage             | Custom metric + alarm                         |
| Analyze Lambda failures                | Logs + Log Insights                           |
| Audit logins & security events         | Integrate CloudTrail logs into CloudWatch     |
| Track performance across microservices | Use metrics, dashboards, and X-Ray (together) |

---

## üü¶ **2. CloudWatch Metrics**

---

### ‚úÖ **What are Metrics in CloudWatch?**

A **metric** in CloudWatch is a **time-ordered set of data points** that represent the **value of a variable** over time ‚Äî like **CPU utilization, memory usage, or number of API calls**.

Each metric consists of:

| Component       | Description                                                                       |
| --------------- | --------------------------------------------------------------------------------- |
| **Namespace**   | Logical container for grouping related metrics (e.g., `AWS/EC2`, `AWS/Lambda`)    |
| **Dimensions**  | Key-value pairs that identify a metric uniquely (e.g., `InstanceId=i-0123456789`) |
| **Metric Name** | The name of the measurement (e.g., `CPUUtilization`, `Duration`)                  |
| **Timestamp**   | The time when the metric data point was recorded                                  |
| **Value**       | The actual numeric data (e.g., 55% CPU usage)                                     |
| **Unit**        | Unit of measurement (Percent, Seconds, Bytes, Count, etc.)                        |

> üìå A metric is uniquely defined by **Namespace + Metric Name + Dimensions**.

---

### üîπ **Built-in (Default) Metrics vs Custom Metrics**

| Type                 | Description                                                               | Example                                         |
| -------------------- | ------------------------------------------------------------------------- | ----------------------------------------------- |
| **Built-in Metrics** | Automatically published by AWS services                                   | `AWS/EC2` ‚Üí `CPUUtilization`, `NetworkIn`, etc. |
| **Custom Metrics**   | Published by you (application-level or on-prem) using `PutMetricData` API | Track queue length in Redis, memory usage, etc. |

‚úÖ You can push custom metrics using:

* **AWS CLI**
* **CloudWatch Agent**
* **SDKs (Python, Go, etc.)**
* **Embedded in your application**

---

### üî∏ **Granularity of Metrics**

| Granularity                    | Description                                                                  | Use Case                                               |
| ------------------------------ | ---------------------------------------------------------------------------- | ------------------------------------------------------ |
| **Standard (1-minute)**        | Most AWS services emit data at 1-minute intervals                            | Good for most monitoring                               |
| **High-resolution (1-second)** | You can publish custom metrics every 1 second (use `--storage-resolution 1`) | Needed for high-frequency apps (e.g., trading, gaming) |

> ‚è± **High-resolution** metrics support up to **sub-minute alarms**, giving you **faster alerting** and **finer visibility**.

---

### üìä **Metric Examples per Service**

Here are **common metrics** across AWS services:

---

#### üñ• **EC2 (Namespace: AWS/EC2)**

| Metric                       | Description          |
| ---------------------------- | -------------------- |
| `CPUUtilization`             | % of CPU used        |
| `NetworkIn / NetworkOut`     | Bytes received/sent  |
| `DiskReadOps / DiskWriteOps` | I/O operations count |
| `StatusCheckFailed`          | Health check status  |

> üîß Use EC2 **Custom Metrics** for memory, disk space (not built-in).

---

#### ‚öôÔ∏è **Lambda (Namespace: AWS/Lambda)**

| Metric        | Description                                     |
| ------------- | ----------------------------------------------- |
| `Invocations` | Count of function calls                         |
| `Duration`    | Function execution time                         |
| `Errors`      | Number of failed invocations                    |
| `Throttles`   | Invocation attempts over concurrency limit      |
| `IteratorAge` | For stream-based triggers like Kinesis/DynamoDB |

---

#### üíæ **EBS (Namespace: AWS/EBS)**

| Metric           | Description                                  |
| ---------------- | -------------------------------------------- |
| `VolumeReadOps`  | Read operations on volume                    |
| `VolumeIdleTime` | Idle time with no ops                        |
| `BurstBalance`   | Remaining IOPS credits for burstable volumes |

---

#### üõ¢ **RDS (Namespace: AWS/RDS)**

| Metric                 | Description                |
| ---------------------- | -------------------------- |
| `CPUUtilization`       | DB instance CPU usage      |
| `DatabaseConnections`  | Active DB connections      |
| `ReadIOPS / WriteIOPS` | Disk read/write operations |

---

#### üåê **ALB (Namespace: AWS/ApplicationELB)**

| Metric                      | Description         |
| --------------------------- | ------------------- |
| `RequestCount`              | Number of requests  |
| `TargetResponseTime`        | Latency from target |
| `HTTPCode_Target_5XX_Count` | Server-side errors  |

---

### üß† **Production Best Practices**

* **Use dimensions wisely** ‚Äì too many unique dimension values can blow up costs and complexity.
* Use **high-resolution metrics** only where **absolutely necessary** (due to cost).
* Set up **dashboards** and **alarms** using key metrics.
* Use **CloudWatch Agent** for custom OS-level metrics (e.g., memory, disk).
* Integrate with **Auto Scaling**, **SNS**, **Lambda** for automation on threshold breaches.

---


## üü¶ **3. CloudWatch Custom Metrics**

---

### ‚úÖ **What are Custom Metrics in CloudWatch?**

**Custom metrics** are metrics that **you push manually** to Amazon CloudWatch ‚Äî they are **not emitted automatically** by AWS services.

You use them when:

* You're monitoring **non-AWS systems** (e.g., on-prem or Docker apps).
* You need **app-specific metrics** (e.g., number of items in a Redis queue, DB transaction times, feature toggle usage, etc.).

---

### üõ†Ô∏è **How to Push Custom Metrics**

#### üß© **Using AWS SDK (or AWS CLI)**

Custom metrics are pushed using the **`PutMetricData`** API.

#### üß™ **CLI Example:**

```bash
aws cloudwatch put-metric-data \
  --namespace "MyApp/QueueService" \
  --metric-name "QueueDepth" \
  --dimensions QueueName=order-events \
  --value 57 \
  --unit Count
```

* **Namespace**: Logical grouping ‚Äî use your own (e.g., `MyApp/ServiceX`)
* **MetricName**: E.g., `TransactionLatency`, `MemoryUsed`
* **Dimensions**: Key-value pairs to identify the metric uniquely
* **Unit**: Seconds, Bytes, Percent, Count, etc.

---

### üß¨ **Using AWS SDK (Python Boto3 Example)**

```python
import boto3

cloudwatch = boto3.client('cloudwatch')

cloudwatch.put_metric_data(
    Namespace='MyApp/CheckoutService',
    MetricData=[
        {
            'MetricName': 'TransactionCount',
            'Dimensions': [
                {'Name': 'Environment', 'Value': 'prod'}
            ],
            'Value': 120,
            'Unit': 'Count'
        }
    ]
)
```

You can schedule this in your app every X seconds/minutes or hook it to specific events.

---

### üß† **Use Cases for Custom Metrics**

| Metric                   | Description                                  |
| ------------------------ | -------------------------------------------- |
| **Queue Depth**          | Track items in SQS, Kafka, RabbitMQ, etc.    |
| **Memory Usage**         | Track memory used by your process            |
| **Transaction Count**    | Total API hits, successful logins, signups   |
| **Cache Hit Ratio**      | Redis or Memcached performance               |
| **Error Rates**          | Track custom app-level errors or retries     |
| **Custom Business KPIs** | E.g., OrdersPlaced, ItemsInCart, AbandonRate |

These are **not available by default** ‚Äî you must explicitly push them.

---

### üï∞Ô∏è **Granularity and Resolution**

* **Default Resolution**: 60 seconds
* **High-Resolution**: Push with `StorageResolution=1` (1-second granularity)

**High-res example in CLI**:

```bash
--storage-resolution 1
```

---

### üí∞ **Pricing Implications**

Custom metrics **cost extra**, especially compared to built-in ones.

| Item                           | Price                                             |
| ------------------------------ | ------------------------------------------------- |
| **Standard resolution metric** | \~\$0.30 per metric per month                     |
| **High-resolution metric**     | \~\$0.30 per metric + per datapoint charges       |
| **Data points**                | \~\$0.01 per 1,000 metrics at 1-second resolution |
| **Alarms on custom metrics**   | Extra (\~\$0.10/alarm/month)                      |

> üìå **Each unique combination of Namespace + MetricName + Dimensions is counted as a separate metric.**

So be cautious with **dimension explosion** (e.g., pushing metrics per user ID or request ID).

---

### üõ°Ô∏è **Production Tips**

* **Batch metrics** to reduce API calls (max 20 per request).
* Use **CloudWatch Agent** for system-level metrics (e.g., memory, disk).
* Consider **CloudWatch Embedded Metric Format (EMF)** if you're using structured logs.
* Use **aggregation/deduplication** in code to avoid pushing too many unnecessary datapoints.
* Watch out for **bill spikes** due to high cardinality metrics (e.g., per container/task).

---

## üü¶ **4. CloudWatch Logs**

---

### ‚úÖ **What is CloudWatch Logs?**

CloudWatch Logs is a **centralized log storage and analysis service** in AWS.

It allows you to:

* Collect and store **logs from AWS services** (like Lambda, ECS, API Gateway).
* Ingest **custom logs** from applications or on-prem systems.
* **Query logs**, **trigger alarms**, and **stream logs to other services** (e.g., S3, Lambda, or ELK).

---

### üì¶ **Key Concepts**

#### üîπ **Log Group**

* A container for logs.
* Typically one **log group per application or service**.
* Example: `/aws/lambda/payment-service` or `/ecs/my-microservice`.

#### üîπ **Log Stream**

* A sequence of log events from **a single source** (e.g., one container or Lambda instance).
* Each Lambda invocation or ECS task generates a new stream.

> üí° Think of a Log Group as a folder and Log Streams as files within it.

---

### üß∞ **Default Log Groups**

Many AWS services **create log groups automatically** when logging is enabled:

| AWS Service                  | Default Log Group             |
| ---------------------------- | ----------------------------- |
| **Lambda**                   | `/aws/lambda/<function-name>` |
| **ECS (Fargate)**            | `/ecs/<task-name>`            |
| **API Gateway (REST)**       | `/aws/apigateway/<stage>`     |
| **CloudTrail (if streamed)** | `/aws/cloudtrail/<region>`    |
| **VPC Flow Logs**            | `/aws/vpc/flow-logs`          |

> Make sure **permissions (IAM)** allow services to write to these groups.

---

### ‚úçÔ∏è **Manual Log Ingestion**

You can send logs manually using:

```bash
aws logs put-log-events \
  --log-group-name <group-name> \
  --log-stream-name <stream-name> \
  --log-events timestamp=<epoch-ms>,message="Your log message"
```

You must:

* Create a **log group** and **log stream** before pushing.
* Track the `uploadSequenceToken` for each log stream.

#### üìå Example Workflow:

```bash
aws logs create-log-group --log-group-name /custom/my-service
aws logs create-log-stream --log-group-name /custom/my-service --log-stream-name my-instance-001
```

Push log data:

```bash
aws logs put-log-events \
  --log-group-name /custom/my-service \
  --log-stream-name my-instance-001 \
  --log-events file://logs.json \
  --sequence-token <token>
```

---

### üí° **Use Cases for CloudWatch Logs**

#### üî∏ **1. Application Logs**

* Your app (Node.js, Python, Java, etc.) pushes logs via SDK or CloudWatch Agent.
* Can use [Embedded Metric Format (EMF)](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format.html) to enable **structured logging + metrics**.

#### üî∏ **2. API Gateway Logs**

* Enable **access and execution logging**.
* Includes request/response, status codes, latency.
* Great for debugging + tracing.

#### üî∏ **3. ECS/Fargate Logs**

* Enable **AWSLogs** or **FireLens** logging in task definitions.
* Logs go to `/ecs/<task-name>`.
* Supports structured logs (e.g., JSON) and routing to S3, Fluentd, etc.

#### üî∏ **4. Lambda Logs**

* All `console.log` / `print` statements automatically captured.
* Automatically grouped by `/aws/lambda/<function-name>`.

#### üî∏ **5. Custom Logs from EC2**

* Install **CloudWatch Agent** to send:

  * `/var/log/syslog`, `/var/log/messages`, NGINX, app logs, etc.
* Configurable via JSON or SSM Parameter Store.

---

### üìä **Visualization + Search**

Use:

* **CloudWatch Logs Insights**: Powerful SQL-like query engine.
* **Log Metric Filters**: Convert patterns in logs into metrics (e.g., `ERROR`, `duration > 500ms`).
* **CloudWatch Dashboards**: Show logs alongside metrics and alarms.

---

### üõ°Ô∏è **Production Considerations**

| Practice                        | Why Important                                           |
| ------------------------------- | ------------------------------------------------------- |
| Enable **retention policy**     | Prevents unlimited log growth & high bills              |
| Use **filter patterns**         | Turn logs into actionable metrics                       |
| Limit **high cardinality logs** | Avoid exploding costs (e.g., log per user ID)           |
| Use **structured logs** (JSON)  | Easier querying, parsing, and alerting                  |
| Enable **encryption (KMS)**     | Secure sensitive log data                               |
| Consider **subscriptions**      | Real-time log streaming to Lambda, S3, OpenSearch, etc. |

---

## üü¶ **5. CloudWatch Agent**

---

### ‚úÖ **What is the CloudWatch Agent?**

The **CloudWatch Agent** is a software component installed on EC2 instances (or on-prem servers) that **collects system-level metrics and logs**, beyond what‚Äôs available by default.

> It enables **detailed host-level monitoring** (like memory, disk, processes, and custom logs) not available through standard EC2 metrics.

---

### üì• **Installing CloudWatch Agent on EC2**

#### üî∏ Step 1: Download the Agent (Amazon Linux/Ubuntu/Windows)

**Amazon Linux 2 / RHEL / CentOS:**

```bash
sudo yum install amazon-cloudwatch-agent
```

**Ubuntu/Debian:**

```bash
wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb
sudo dpkg -i amazon-cloudwatch-agent.deb
```

**Windows:**
Download from the [CloudWatch Agent downloads page](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-commandline.html#windows-download-command).

---

### üîê **Step 2: IAM Permissions**

Attach an **IAM role** to your EC2 instance with this policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "cloudwatch:PutMetricData",
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "ssm:GetParameter"
      ],
      "Resource": "*"
    }
  ]
}
```

> You can scope the `Resource` field if needed for stricter control.

---

### üßæ **amazon-cloudwatch-agent.json (Config File)**

This JSON file controls what metrics and logs to collect.

#### üî∏ Example: Collect Memory, Disk, and Logs

```json
{
  "metrics": {
    "metrics_collected": {
      "mem": {
        "measurement": [
          "mem_used_percent"
        ]
      },
      "disk": {
        "resources": ["/"],
        "measurement": [
          "used_percent"
        ]
      }
    },
    "append_dimensions": {
      "InstanceId": "${aws:InstanceId}"
    }
  },
  "logs": {
    "logs_collected": {
      "files": {
        "collect_list": [
          {
            "file_path": "/var/log/syslog",
            "log_group_name": "/ec2/syslog",
            "log_stream_name": "{instance_id}"
          }
        ]
      }
    }
  }
}
```

---

### üîß **Configure and Start Agent**

#### üìå Option 1: Local config file

```bash
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \
  -a fetch-config \
  -m ec2 \
  -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json \
  -s
```

#### üìå Option 2: Use SSM Parameter Store (for central config)

1. Upload config:

```bash
aws ssm put-parameter \
  --name "AmazonCloudWatch-agent-config" \
  --type "String" \
  --value file://config.json
```

2. Start agent using SSM reference:

```bash
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \
  -a fetch-config \
  -m ec2 \
  -c ssm:AmazonCloudWatch-agent-config \
  -s
```

---

### üìä **Metrics You Can Collect**

| Metric Type     | Examples                                                        |
| --------------- | --------------------------------------------------------------- |
| **Memory**      | `mem_used_percent`, `mem_available`, `swap_used_percent`        |
| **Disk**        | `disk_used_percent`, `inodes_free`, `read_bytes`, `write_bytes` |
| **CPU**         | `cpu_usage_idle`, `cpu_usage_iowait`                            |
| **Network**     | `bytes_sent`, `bytes_recv`, `packets_dropped`                   |
| **Processes**   | `processes_running`, `processes_blocked`                        |
| **Custom logs** | Application logs (e.g., NGINX, SpringBoot logs)                 |

---

### üß† **Use Cases for Hybrid / Bare-Metal Monitoring**

The CloudWatch Agent also supports:

* **On-prem servers**
* **Hybrid environments**
* **Bare-metal EC2 instances**
* **Edge devices**

Using **AWS Systems Manager (SSM)** with hybrid activation, you can:

* Install the CloudWatch Agent on non-EC2 servers.
* Collect system logs and metrics.
* Centralize monitoring for both **cloud + on-prem workloads**.

#### üìå Common Hybrid Use Cases:

* Monitoring **legacy data centers** alongside EC2.
* Collecting **OS-level metrics from Kubernetes worker nodes**.
* Monitoring **non-AWS edge devices** (IoT, retail stores, etc.).
* Correlating **application metrics with infrastructure health**.

---

### üí∞ **Pricing Notes**

* **CloudWatch Agent metrics** are billed under **custom metrics**.

  * First 10K metrics/month are **free** (as per current AWS Free Tier).
  * Then **\$0.30 per metric per month** (for standard 1-minute resolution).
* **High-resolution metrics (1-second)** cost more.
* **Logs** are billed by **ingested GB** and **retention duration**.

---

## üü¶ **6. Logging for ECS/Fargate, EKS, Lambda**

---

### üîπ **A. ECS / Fargate Logging**

#### ‚úÖ **Default Logging Options**

You can direct logs from containers to:

* **CloudWatch Logs** *(default)*
* **FireLens** *(Fluent Bit / Fluentd)*
* **Third-party destinations** (e.g., Datadog, Elasticsearch, S3)

#### üî∏ 1. **CloudWatch Logs via awslogs driver**

In your ECS Task Definition:

```json
"logConfiguration": {
  "logDriver": "awslogs",
  "options": {
    "awslogs-group": "/ecs/my-app",
    "awslogs-region": "ap-south-1",
    "awslogs-stream-prefix": "my-service"
  }
}
```

This sends logs from containers to:

```
/ecs/my-app
   ‚îî‚îÄ‚îÄ my-service/container-id
```

---

#### üî∏ 2. **FireLens + Fluent Bit for Advanced Logging**

Use [**FireLens**](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_firelens.html) when:

* You need **structured JSON log processing**
* You want to **send logs to multiple destinations** (e.g., CloudWatch + S3)
* You want **custom filtering/formatting**

FireLens uses **Fluent Bit** under the hood and runs as a sidecar.

**Sample Task Definition Snippet**:

```json
"logConfiguration": {
  "logDriver": "awsfirelens",
  "options": {
    "Name": "cloudwatch",
    "region": "ap-south-1",
    "log_group_name": "/ecs/firelens-app",
    "auto_create_group": "true"
  }
}
```

---

### üîπ **B. EKS Logging**

#### ‚úÖ **Container Insights (Metrics + Logs)**

Enabling **CloudWatch Container Insights** for EKS helps collect:

* **Node, pod, container metrics**
* **Structured logs from stdout/stderr**
* **Performance logs** (disk, network, memory)

##### üì¶ Components Used:

* **CloudWatch Agent**: Collects metrics
* **Fluent Bit**: Collects logs
* **DaemonSets**: Deployed to each node

#### üîß Setup with `eksctl`:

```bash
eksctl utils update-cluster-logging \
  --enable-types all \
  --region ap-south-1 \
  --cluster my-cluster
```

#### üìÅ Logs Structure:

```
/aws/containerinsights/cluster-name/performance
/aws/containerinsights/cluster-name/application
```

Use filters like:

```bash
fields @timestamp, @message
| filter kubernetes.container_name = "my-app"
```

> üîç Best for full-cluster observability.

---

### üîπ **C. Lambda Logging**

#### ‚úÖ **Default Behavior**

* **Each Lambda function** has a **log group automatically created** in CloudWatch Logs:

  ```
  /aws/lambda/function-name
  ```

* Every function **invocation** writes logs via `console.log()` (Node.js), `print()` (Python), etc.

#### üîÅ **Log Stream Structure:**

Each new container invocation creates a new log stream:

```
/aws/lambda/my-function
   ‚îî‚îÄ‚îÄ 2025/08/08/[LATEST]1234abcdef5678
```

---

#### ‚è≥ **Retention Configuration**

By default, logs are **kept forever** unless manually changed.

You can set log group retention with CLI or Terraform:

```bash
aws logs put-retention-policy \
  --log-group-name /aws/lambda/my-function \
  --retention-in-days 14
```

Or enable this in the Lambda function settings.

---

### üìå **Comparison Summary**

| Platform        | Default Logging Destination              | Custom Logging                               | Retention Control       |
| --------------- | ---------------------------------------- | -------------------------------------------- | ----------------------- |
| **ECS/Fargate** | `/ecs/*` log groups via `awslogs` driver | FireLens + Fluent Bit                        | Yes (log group setting) |
| **EKS**         | `/aws/containerinsights/*`               | Fluent Bit, Loki, etc.                       | Yes                     |
| **Lambda**      | `/aws/lambda/*`                          | Extensions possible via Lambda Telemetry API | Yes                     |

---




## üü¶ **7. CloudWatch Log Insights**

---

### üîπ **What is CloudWatch Log Insights?**

Amazon **CloudWatch Logs Insights** is a **fully managed, interactive log analytics service** that lets you run **real-time queries** on your CloudWatch Logs using a **SQL-like syntax**.

* Query logs **without exporting** to another tool.
* Explore **patterns, anomalies, error trends**.
* Perform **aggregations, parsing, filtering**, and **visualizations**.

---

### üî∏ **Key Features**

* Near real-time querying of logs
* SQL-like DSL (domain-specific language)
* Support for **`parse`, `filter`, `stats`, `sort`, `limit`, `fields`**
* Supports logs from:

  * Lambda
  * ECS/Fargate
  * EKS (via Fluent Bit)
  * EC2 apps
  * API Gateway
  * ALB, VPC Flow Logs, etc.

---

### üîπ **Basic Syntax Structure**

```sql
fields @timestamp, @message
| filter @message like /ERROR/
| sort @timestamp desc
| limit 20
```

#### Key Built-in Fields:

| Field        | Description                |
| ------------ | -------------------------- |
| `@timestamp` | Time of the log event      |
| `@message`   | Raw log message            |
| `@logStream` | Stream from which log came |
| `@log`       | Log group name             |

---

### üîπ **Common Clauses**

| Clause   | Usage Example                                     |
| -------- | ------------------------------------------------- |
| `filter` | `filter @message like /timeout/`                  |
| `fields` | `fields status, latency`                          |
| `stats`  | `stats count(*) by status`                        |
| `sort`   | `sort @timestamp desc`                            |
| `limit`  | `limit 50`                                        |
| `parse`  | `parse @message "* status=* latency=*ms" as s, l` |

---

### üîπ **Use Cases**

#### ‚úÖ **1. Trace Errors in Microservices**

```sql
fields @timestamp, @message, @logStream
| filter @message like /ERROR|Exception/
| sort @timestamp desc
| limit 50
```

* Identify failures across services like Lambda, ECS, etc.
* Cross-reference container names via `@logStream`.

---

#### ‚úÖ **2. Analyze API Latency Patterns**

For structured logs like:

```
GET /api/user status=200 latency=52ms
```

You can:

```sql
parse @message "* status=* latency=*ms" as method, status, latency
| stats avg(latency) by status
```

#### Or group by API path:

```sql
parse @message "* * latency=*ms" as method, path, latency
| stats avg(latency), max(latency), count(*) by path
```

---

#### ‚úÖ **3. Count Lambda Invocations per Hour**

```sql
stats count(*) by bin(1h)
```

---

#### ‚úÖ **4. Detect 5xx Errors in ALB**

```sql
fields @timestamp, target_status_code
| filter target_status_code >= 500
| stats count(*) by target_status_code
```

---

### üîπ **Best Practices**

* Use `limit` to preview before heavy queries
* Always `filter` early to reduce scanned data
* Combine `parse` and `stats` for powerful analytics
* Consider `bin()` to group by time intervals

---

### üî∏ **Visualizations**

You can save a query as a **CloudWatch Dashboard widget** (line chart, bar chart, etc.) to create live visual reports.

---

## üü• **8. CloudWatch Alarms**

---

### üî∏ **What is a CloudWatch Alarm?**

A **CloudWatch Alarm** watches a **CloudWatch metric (or math expression)** and triggers actions based on **threshold conditions**.

Alarms are used for:

* **Alerting**
* **Automation**
* **Health monitoring**
* **Resource recovery or scaling**

---

### üîπ **1. Metric-based Threshold Alerts**

You define alarms based on:

* A **single metric** (e.g., CPUUtilization)
* A **math expression** (e.g., combining metrics)

Basic alarm components:

* **Metric Name**
* **Namespace** (e.g., `AWS/EC2`)
* **Statistic** (Average, Sum, Min, Max)
* **Period** (evaluation interval, like 1 min)
* **Threshold** (when to trigger)
* **Comparison** (`GreaterThanThreshold`, `LessThanThreshold`)

#### üß† Example:

```txt
Alarm: High CPU on WebServer
Metric: AWS/EC2 - CPUUtilization
Threshold: > 80%
Evaluation Periods: 3 √ó 1-minute
Action: Send alert to SNS
```

---

### üîπ **2. Static vs Anomaly Detection Alarms**

| Type                  | Description                                                              |
| --------------------- | ------------------------------------------------------------------------ |
| **Static Alarm**      | Fixed threshold (e.g., `> 80%`)                                          |
| **Anomaly Detection** | Uses ML to create a **dynamic band** of ‚Äúnormal‚Äù behavior for the metric |

#### üîç Example of Anomaly Detection:

If `CPUUtilization` is usually 10‚Äì40% but suddenly spikes to 70%, an anomaly alarm will trigger even if your static threshold is 90%.

> üîß To enable: use `ANOMALY_DETECTION_BAND(metric)` in the alarm expression.

---

### üîπ **3. Alarm Actions**

CloudWatch Alarms can automatically trigger:

| Action Type                 | Description                                     |
| --------------------------- | ----------------------------------------------- |
| **SNS**                     | Send email, SMS, mobile push, or trigger Lambda |
| **EC2 Auto Recovery**       | Automatically recover failed EC2 instance       |
| **Auto Scaling Policy**     | Increase/decrease ASG instances                 |
| **Systems Manager OpsItem** | Create operational work items                   |
| **Lambda Function**         | Trigger remediation or custom logic             |

#### üîî Example Action:

If EBS read latency exceeds 100 ms:

```txt
Trigger SNS ‚Üí Notify SRE ‚Üí Auto scale RDS read replica
```

---

### üîπ **4. Composite Alarms (Multi-Metric Logic)**

**Composite alarms** combine **multiple alarms** using **boolean logic** (AND, OR, NOT).

* They **don't monitor metrics directly**
* Instead, they monitor **states of other alarms**
* Useful for reducing noise or creating **fail-safe logic**

#### ‚úÖ Example Use Case:

```txt
Alarm A: CPUUtilization > 85% for 3 min
Alarm B: MemoryUsage > 80% for 3 min

Composite Alarm C = A AND B
‚Üí Trigger only when both CPU and Memory are high
```

#### ‚úÖ Use Case: Reduce alert noise

* Multiple service alarms ‚Üí 1 high-level composite alert

---

### ‚úÖ **Best Practices**

* **Use anomaly detection** where thresholds are unpredictable
* **Add SNS action** to notify Slack/email
* **Use composite alarms** to prevent false positives
* **Set appropriate periods** (e.g., `5 x 1-min` to reduce flapping)
* **Tag alarms** for environment/team ownership

---

## üü™ **9. CloudWatch Anomaly Detection**

---

### üî∏ **What is CloudWatch Anomaly Detection?**

**Anomaly Detection** in CloudWatch uses **machine learning (ML)** to automatically model the **expected behavior of a metric** over time and detect **anomalies**.

Instead of manually setting static thresholds (like CPU > 80%), you let CloudWatch **learn the normal range** and alert you only when the metric **deviates abnormally**.

---

### üîπ **1. Machine Learning-Based Metric Baselining**

CloudWatch applies an ML algorithm to **past metric data** and builds a **normal range (band)**.

This model considers:

* Daily/weekly patterns
* Seasonal spikes (e.g., traffic at 9 AM)
* Long-term trends

#### üìà The result:

* A **normal band** is visualized in dashboards and alarms
* Any deviation outside the band is considered **anomaly**

---

### üîπ **2. Auto-Adaptive Thresholds**

These are **not static**. The model keeps updating as:

* More data comes in
* Usage patterns change

This solves issues with:

* False alarms in auto-scaling workloads
* Missing real issues because of outdated static thresholds

---

### üîπ **3. When to Use It**

Ideal for metrics with:

* Variable baselines
* Spiky, hard-to-define behavior
* No clear upper/lower limits

---

#### üß† Examples:

| Use Case                 | Metric                          | Why Use Anomaly Detection                                                |
| ------------------------ | ------------------------------- | ------------------------------------------------------------------------ |
| **Memory Leak**          | MemoryUsage                     | Slowly increasing usage ‚Üí traditional static alarms won‚Äôt catch it early |
| **DDoS Attack or Spike** | NetworkIn / HTTP 5xx count      | Sudden traffic spikes outside normal                                     |
| **ECS CPU Usage**        | CPUUtilization across services  | Detect unexpected patterns                                               |
| **API Error Rates**      | 5xx/4xx errors on ALB or API GW | Identify abnormal increases                                              |

---

### üîπ **How to Set Up Anomaly Detection**

You can add anomaly detection to:

* Alarms
* Dashboards

#### üß© Syntax for alarms:

```bash
ANOMALY_DETECTION_BAND([metric])
```

#### Example in CLI:

```bash
aws cloudwatch put-anomaly-detector \
  --namespace "AWS/EC2" \
  --metric-name "CPUUtilization" \
  --statistic "Average" \
  --dimensions Name=InstanceId,Value=i-1234567890abcdef0
```

Or via **console** ‚Üí when creating an alarm ‚Üí enable "Anomaly Detection" for metric.

---

### üîπ **Limitations**

* Not available for math expressions (only raw metrics)
* Needs **enough historical data** (ideally > 2 weeks)
* Some AWS services don't yet support it for all metrics

---

### ‚úÖ Best Practices

* Use with **cloud workloads that have dynamic behavior**
* Add **anomaly-based alarms** alongside static ones for full coverage
* Visualize the **detection band** in dashboards to correlate spikes
* Combine with **SNS alerts** or **composite alarms** to act on anomalies

---

## üü© **10. CloudWatch Dashboards**

---

### üî∏ **What Are CloudWatch Dashboards?**

**Amazon CloudWatch Dashboards** are **customizable, real-time visual interfaces** that let you:

* Monitor metrics across services in **one view**
* Mix metrics, graphs, logs, and text
* View data across **regions and accounts** (via cross-account setups)

They‚Äôre ideal for operations, SRE, and DevOps teams to track system health, app behavior, and anomalies **at a glance**.

---

### üîπ **1. Real-Time Visual Monitoring**

Dashboards provide near real-time visibility for:

* Infrastructure: EC2, EBS, ALB, VPC, etc.
* Application services: Lambda, API Gateway, RDS, ECS
* Custom metrics (e.g., app-specific counters)
* Alarms, status indicators, and trends

You can display:

* Line graphs
* Number widgets (single stat)
* Text notes
* Logs (using Insights or links)

üîß **Update Frequency**: Every 1‚Äì2 minutes (based on metric resolution)

---

### üîπ **2. Combine Metrics, Graphs, Logs**

Each **dashboard** is made of **widgets**:

* **Metric Widgets** ‚Üí Line or stacked graphs
* **Number Widgets** ‚Üí Show real-time values (e.g., latency, error count)
* **Text Widgets** ‚Üí Add descriptions or instructions
* **Log Widgets** ‚Üí Run **CloudWatch Log Insights queries**, show results inline

‚úÖ You can mix multiple services and namespaces in one dashboard:

* EC2 CPU + Lambda Duration + S3 4xx errors + RDS CPUUtilization

üß† **Use Case**: Create a microservices-wide dashboard with metrics from all critical components.

---

### üîπ **3. Cross-Region Dashboard Support**

By default, CloudWatch Dashboards are **regional**.

But:

* You can **add widgets from other AWS regions** into one dashboard
* Helps in managing **multi-region** apps or DR setups

üìå To add metrics from other regions:

* Select a widget
* Choose the metric
* Pick the desired region

---

### üîπ **4. JSON Structure Behind Dashboards**

CloudWatch Dashboards are actually defined using a **JSON document**.

#### You can:

* Export dashboards as JSON
* Automate them with Terraform/CDK/CloudFormation
* Store versioned dashboards in Git

#### Example Widget JSON:

```json
{
  "widgets": [
    {
      "type": "metric",
      "x": 0,
      "y": 0,
      "width": 6,
      "height": 6,
      "properties": {
        "metrics": [
          [ "AWS/EC2", "CPUUtilization", "InstanceId", "i-12345678" ]
        ],
        "period": 300,
        "stat": "Average",
        "region": "us-east-1",
        "title": "EC2 CPU Usage"
      }
    }
  ]
}
```

You can define:

* Widget layout (`x`, `y`, `width`, `height`)
* Metric config: namespace, name, dimensions
* Time range, region, title

üìç **Automation Idea**: Store JSON files in Git ‚Üí deploy via `aws cloudwatch put-dashboard` in CI/CD.

---

### üîπ **Useful CLI/SDK Commands**

```bash
# Create or update dashboard
aws cloudwatch put-dashboard \
  --dashboard-name "MyApp-Dashboard" \
  --dashboard-body file://dashboard.json

# Get dashboard JSON
aws cloudwatch get-dashboard \
  --dashboard-name "MyApp-Dashboard"
```

---

### ‚úÖ Best Practices

* **Group by microservice** or system (e.g., API, DB, Auth, Cache)
* Use **Log Insights widgets** for real-time logs
* Use **single-value widgets** to show key business metrics
* Automate dashboard creation across environments (dev/staging/prod)
* Use **cross-region widgets** for global visibility

---

## üü¶ **11. CloudWatch Events (Now EventBridge)**

---

### üî∏ **What Is CloudWatch Events / EventBridge?**

**Amazon EventBridge** (formerly **CloudWatch Events**) is a **serverless event bus** that:

* Delivers **real-time events** from AWS services, SaaS providers, or custom apps
* Supports **event filtering**, **routing**, and **automation**
* Is used to respond to state changes in your environment or to **schedule tasks**

EventBridge is a **powerful automation tool** that connects **event producers** (e.g., EC2, ECS, CodePipeline) to **event consumers** (e.g., Lambda, SQS, SNS).

---

### üîπ **1. Monitor AWS API Calls & State Changes**

EventBridge can listen to events like:

* EC2 instance launched/stopped
* ECS task failed
* CodePipeline state changed
* IAM role updated
* EBS volume attached/detached

These events are emitted **automatically** by AWS services and can be routed to targets for automation.

üß† Use cases:

* Auto-tagging newly created EC2 instances
* Triggering a Lambda on pipeline failure
* Restarting ECS tasks on failure
* Alerting when an IAM policy is changed

---

### üîπ **2. Scheduled Events (Cron + Rate)**

You can **schedule tasks using EventBridge** without needing cron on servers.

#### üî∏ Syntax Examples:

```cron
cron(0 3 * * ? *)         ‚Üí every day at 3 AM UTC
rate(1 hour)              ‚Üí every hour
rate(5 minutes)           ‚Üí every 5 minutes
```

üõ† Common uses:

* Scheduled backups
* Periodic cleanup jobs
* Trigger Lambda functions on a daily/hourly basis
* Schedule ECS tasks or Step Functions

üí° Behind the scenes, **EventBridge + Lambda** = a serverless cron system.

---

### üîπ **3. Rule Pattern Matching**

EventBridge rules can **filter and match** event patterns to determine when to trigger targets.

#### üî∏ Example 1: Match EC2 start events

```json
{
  "source": ["aws.ec2"],
  "detail-type": ["EC2 Instance State-change Notification"],
  "detail": {
    "state": ["running"]
  }
}
```

#### üî∏ Example 2: Match CodePipeline failure

```json
{
  "source": ["aws.codepipeline"],
  "detail-type": ["CodePipeline Pipeline Execution State Change"],
  "detail": {
    "state": ["FAILED"]
  }
}
```

You can use:

* `source` ‚Üí AWS service (e.g., `aws.ecs`, `aws.lambda`)
* `detail-type` ‚Üí Event type
* `detail` ‚Üí Match event-specific fields (state, region, ID)

üìå Rules are **declarative filters**, not procedural code.

---

### üîπ **Targets for Event Rules**

When a rule matches an event, it can **send the event to a target**, such as:

* **Lambda function**
* **SNS topic**
* **SQS queue**
* **Step Functions**
* **Kinesis stream**
* **EC2 Systems Manager (SSM) Run Command**
* **ECS task**
* **CloudWatch log group**
* **API destinations** (external webhooks)

You can even transform the event payload using **InputTransformer** before sending it to the target.

---

### üîπ **Multiple Event Buses**

There are:

* **Default event bus** ‚Üí receives all AWS events
* **Custom event buses** ‚Üí receive app or partner events

You can send custom events to custom buses using:

```bash
aws events put-events --entries file://events.json
```

---

### ‚úÖ Use Cases

| Use Case                 | Description                                       |
| ------------------------ | ------------------------------------------------- |
| üîÅ Auto-restart ECS task | Listen for ECS task failure ‚Üí trigger new task    |
| üéØ CI/CD event hook      | Trigger Lambda on CodePipeline stage failure      |
| üìÜ Scheduled job         | Run cleanup Lambda every day at midnight          |
| üõë Security              | Trigger alert on IAM policy change                |
| üåé Cross-account events  | Route events across AWS accounts with EventBridge |

---

### üõ† Example CLI

```bash
aws events put-rule \
  --name "dailyLambdaTrigger" \
  --schedule-expression "rate(1 day)"

aws events put-targets \
  --rule "dailyLambdaTrigger" \
  --targets "Id"="1","Arn"="arn:aws:lambda:REGION:ACCOUNT_ID:function:MyFunction"
```

---

## üü¶ **12. CloudWatch Logs Subscription Filters**

---

### üî∏ **What Are Subscription Filters?**

**CloudWatch Logs Subscription Filters** allow you to **stream log data in near real-time** to:

* **AWS Lambda**
* **Amazon Kinesis Data Streams**
* **Amazon Kinesis Data Firehose**

This is useful for:

* **Real-time log processing**
* **Security analysis**
* **Custom alerting**
* **Analytics pipelines**

---

### üîπ **1. Real-time Log Forwarding Destinations**

#### üî∏ Lambda

‚Üí Process logs in real time (e.g., parse, alert, enrich, or route logs).
Common for **alerting on specific messages** or **triggering workflows**.

#### üî∏ Kinesis Data Streams

‚Üí Send logs to custom consumers (e.g., self-managed apps, log aggregators).

#### üî∏ Kinesis Data Firehose

‚Üí Stream logs to **S3**, **Elasticsearch**, **OpenSearch**, **Splunk**, **DataDog**, or **Redshift**.

---

### üîπ **2. Pattern-Based Filtering**

When setting up a subscription filter, you define a **filter pattern** to match specific log content.

#### üî∏ Examples:

| Goal                              | Filter Pattern                               |
| --------------------------------- | -------------------------------------------- |
| Match all logs                    | `""` (empty string)                          |
| Match lines with ‚ÄúERROR‚Äù          | `"ERROR"`                                    |
| Match lines with 500 status codes | `"500"`                                      |
| Match structured JSON logs        | `{ $.statusCode = 500 }`                     |
| Match logs with fields            | `[ip, user, request, status_code=500, size]` |

Patterns can:

* Match **text**
* Match **JSON structures**
* Match **specific field values**

‚úÖ Supports **simple wildcards and logic**
‚ùå Doesn‚Äôt support regex (use Lambda for complex parsing)

---

### üîπ **3. Use Case: Alert on ‚ÄúERROR‚Äù or HTTP 500s**

Let‚Äôs say your app logs HTTP requests like this:

```json
{"path":"/api/login", "statusCode":500, "message":"Internal Server Error"}
```

You can set a filter:

```json
{ $.statusCode = 500 }
```

This filter will trigger the Lambda whenever a log line contains `statusCode = 500`.

Then, in the Lambda, you could:

* Send alert to Slack/SNS
* Log enriched data to another system
* Trigger an automated remediation workflow

---

### üîπ **4. Setup Process (Steps)**

1. **Create a Lambda/Kinesis/Firehose destination**
2. **Grant `logs:PutSubscriptionFilter` permission**
3. **Create a subscription filter**:

```bash
aws logs put-subscription-filter \
  --log-group-name "/ecs/my-app" \
  --filter-name "error-alert-filter" \
  --filter-pattern "ERROR" \
  --destination-arn "arn:aws:lambda:region:account-id:function:ProcessLogs" \
  --role-arn "arn:aws:iam::account-id:role/CWLtoLambdaRole"
```

---

### üîπ **5. Key Concepts**

| Concept                            | Description                                                        |
| ---------------------------------- | ------------------------------------------------------------------ |
| **Filter pattern**                 | Expression to match log lines                                      |
| **Destination**                    | Lambda/Kinesis/Firehose endpoint                                   |
| **IAM role**                       | Needed if the destination is a Lambda function                     |
| **One subscription per log group** | You can only have **one active subscription filter** per log group |

---

### üß† Best Practices

* Use structured logs (JSON) to make filtering easier
* Use filters to pre-process log volume before sending to analytics/destinations
* Combine with **CloudWatch Logs Insights** for historical analysis

---

### ‚úÖ Real-World Use Cases

| Use Case        | Description                                                 |
| --------------- | ----------------------------------------------------------- |
| üîî Alerting     | Trigger SNS/Lambda on error keywords                        |
| üîí Security     | Detect suspicious IPs or access patterns in logs            |
| üì¶ ETL          | Send filtered logs to S3 via Firehose for long-term storage |
| üìä Analytics    | Stream to OpenSearch for near real-time dashboards          |
| üí¨ Slack Alerts | Lambda + webhook for instant notifications                  |




## üü¶ **13. CloudWatch Logs Retention & Lifecycle**

---

### üî∏ **1. Default Log Retention**

* By **default**, CloudWatch Logs **do not expire**.
* Logs are **stored indefinitely** unless you manually configure a **retention policy**.
* This can lead to **high storage costs**, especially in busy environments (EKS, Lambda, ECS, etc.).

---

### üîπ **2. Setting Retention Policies**

Retention settings are **per log group** and control **how long logs are kept** before automatic deletion.

#### ‚è≥ Available retention periods:

* 1 day
* 3 days
* 5 days
* 1 week (7 days)
* 2 weeks (14 days)
* 1 month (30 days)
* 2 months (60 days)
* 3 months (90 days)
* 6 months (180 days)
* 1 year (365 days)
* 13 months (400 days)
* 18 months (545 days)
* 2 years (731 days)
* 5 years (1827 days)
* 10 years (3653 days)
* **Never expire**

---

### üî∏ **3. Best Practices**

| Environment        | Suggested Retention                                   |
| ------------------ | ----------------------------------------------------- |
| üß™ Dev/Test        | 3‚Äì14 days                                             |
| üè≠ Staging         | 14‚Äì30 days                                            |
| üöÄ Production      | 30‚Äì90 days (longer if compliance requires)            |
| üîê Security logs   | 90+ days or archive to S3                             |
| üì¶ Regulatory data | Consider exporting to **S3 Glacier** for cold storage |

> üß† Tip: Combine **short CloudWatch retention** + **Kinesis Firehose ‚Üí S3** to reduce costs and maintain long-term logs.

---

### üîπ **4. Setting Retention via AWS CLI**

```bash
aws logs put-retention-policy \
  --log-group-name "/aws/lambda/my-function" \
  --retention-in-days 14
```

Use in automation, scripts, or CI/CD pipelines to enforce consistency.

---

### üî∏ **5. Automate Log Group Retention Setup with Lambda**

You can deploy a **Lambda function** to:

* Periodically check for **new log groups**
* Automatically apply a **retention policy**

#### ‚úÖ Use case:

When new Lambda functions or ECS/Fargate services are deployed, they create log groups **without retention**. This function fixes that.

#### üõ† Example: Python Lambda Code (Boto3)

```python
import boto3

LOG_RETENTION_DAYS = 30
client = boto3.client('logs')

def lambda_handler(event, context):
    paginator = client.get_paginator('describe_log_groups')
    for page in paginator.paginate():
        for group in page['logGroups']:
            name = group['logGroupName']
            if 'retentionInDays' not in group:
                print(f"Setting retention for {name}")
                client.put_retention_policy(
                    logGroupName=name,
                    retentionInDays=LOG_RETENTION_DAYS
                )
```

* Trigger this on a **schedule (EventBridge rule)** (e.g., every 6 or 12 hours)
* Add IAM permissions: `logs:DescribeLogGroups` and `logs:PutRetentionPolicy`

---

### üîπ **6. Monitoring & Alerts for Oversized Logs**

You can set **CloudWatch Alarms** to:

* Alert when a log group exceeds a **storage threshold**
* Detect excessive ingestion (`IncomingBytes`) or usage

> Use this to avoid surprise bills due to unbounded logs in noisy services.

---

### üî∏ **7. Alternative: Archive Logs to S3**

For long-term or compliance retention:

* Use **CloudWatch Subscription Filter** ‚Üí **Kinesis Firehose** ‚Üí **S3**
* Store logs for **7+ years** at a fraction of CloudWatch cost
* Use tools like **Athena** or **OpenSearch** to query archived logs

---

### üß† Summary

| Feature              | Benefit                          |
| -------------------- | -------------------------------- |
| ‚úÖ Retention policies | Avoid high storage bills         |
| üõ† Lambda automation | Enforce retention globally       |
| üßä Export to S3      | Enable cold storage & compliance |
| üìä Alerts on usage   | Detect unbounded growth          |



## üü¶ **14. Integrations with Third-Party Tools**

CloudWatch integrates with various third-party and AWS-native analytics tools for extended log storage, visualization, and querying.

---

### üî∏ **1. CloudWatch ‚Üí ELK Stack via Kinesis Firehose**

If your organization uses the **ELK Stack (Elasticsearch, Logstash, Kibana)**, you can forward CloudWatch Logs for better analytics and retention.

#### ‚úÖ Use Case:

* Advanced search, aggregation, and long-term log storage in Elasticsearch
* Centralized logging across hybrid environments

#### üõ† Setup:

1. **Create a Kinesis Data Firehose Delivery Stream**:

   * Destination: **Amazon Elasticsearch Service** or **self-managed Elasticsearch**
   * Optional: Buffer logs to **S3** as backup

2. **Create a CloudWatch Logs Subscription Filter**:

   ```bash
   aws logs put-subscription-filter \
     --log-group-name "/aws/lambda/app" \
     --filter-name "ForwardToFirehose" \
     --filter-pattern "" \
     --destination-arn arn:aws:firehose:region:acct-id:deliverystream/my-stream
   ```

3. **Logs are batched and transformed** (optionally) before indexing in ELK.

> üîí Firehose can transform logs using a Lambda function before delivery.

---

### üîπ **2. Export CloudWatch Logs to S3 ‚Üí Query with Athena**

This approach gives you **SQL-based querying** on logs **without vendor lock-in**.

#### ‚úÖ Use Case:

* Cost-effective long-term storage
* Query old logs across services (Lambda, EKS, ECS)
* Ad-hoc analytics on logs at scale

#### üõ† Steps:

1. **Set up Export Task (Manual or Automated)**:

   ```bash
   aws logs create-export-task \
     --log-group-name "/aws/lambda/my-fn" \
     --from 1620000000000 \
     --to 1620086400000 \
     --destination "my-s3-bucket" \
     --destination-prefix "lambda-logs"
   ```

2. **Logs are exported in GZIP-compressed JSON format**.

3. **Create a Glue Table for Athena**:

   * Use AWS Glue to crawl exported logs and infer schema
   * Or define the schema manually using a CREATE TABLE query

4. **Query logs using Athena**:

   ```sql
   SELECT * FROM cloudwatch_logs
   WHERE message LIKE '%ERROR%'
   AND log_stream LIKE '%prod%'
   LIMIT 100;
   ```

> üí° You can use **Partition Projection** to optimize performance and cost.

---

### üî∏ **3. Grafana Dashboards via CloudWatch Plugin**

Grafana supports **CloudWatch as a data source** directly using the **AWS CloudWatch plugin**.

#### ‚úÖ Use Case:

* Unified dashboards with CloudWatch, Prometheus, and others
* Real-time metric visualization
* Cross-service performance observability

#### üõ† Setup:

1. **Create IAM Role or User** with:

   * `cloudwatch:ListMetrics`, `GetMetricData`, `GetMetricStatistics`, etc.

2. **Configure Grafana Data Source**:

   * Go to: **Settings > Data Sources > Add CloudWatch**
   * Provide credentials (Access keys, AssumeRole via IRSA, etc.)

3. **Build Dashboards**:

   * Use CloudWatch namespaces like `AWS/EC2`, `AWS/ECS`, `ContainerInsights`, etc.
   * Mix metrics and logs panels

4. **Add Logs Panel via Loki or CloudWatch Logs Insights**:

   * Use CloudWatch Logs Insights queries directly in Grafana panels
   * e.g., Show failed Lambda invocations

#### üß† Example Dashboard Widgets:

| Metric                    | Description                         |
| ------------------------- | ----------------------------------- |
| `AWS/Lambda:Duration`     | Track avg/max Lambda execution time |
| `AWS/ECS:CPUUtilization`  | CPU usage of ECS services           |
| `AWS/ApiGateway:4XXError` | API error rate                      |
| `Logs Insights`           | Top errors by service name          |

---

### üîπ Summary Table

| Integration                   | Purpose        | Benefit                                           |
| ----------------------------- | -------------- | ------------------------------------------------- |
| CloudWatch ‚Üí ELK via Firehose | Log forwarding | Advanced log search and dashboards                |
| Export to S3 ‚Üí Athena         | Log archiving  | Cost-effective querying with SQL                  |
| Grafana via CloudWatch plugin | Dashboards     | Unified observability, real-time metrics and logs |


---


## üü™ **15. CloudWatch vs X-Ray**

Understanding the distinction and relationship between **CloudWatch** and **AWS X-Ray** is crucial, especially for **microservices** observability.

---

### üî∏ **What is CloudWatch?**

CloudWatch is AWS‚Äôs monitoring and observability suite that covers:

* **Metrics**: CPU, memory, latency, custom application metrics.
* **Logs**: Application logs, system logs, VPC flow logs, etc.
* **Dashboards, Alarms, Events** for alerting and visualization.

#### ‚úÖ Best For:

* **Infrastructure health** (CPU, disk, network)
* **Application logs & custom metrics**
* **Alerting** and automated responses (SNS, Lambda)
* **Cost optimization** using metrics over time

---

### üî∏ **What is AWS X-Ray?**

X-Ray is AWS‚Äôs **distributed tracing system** designed to:

* Trace **end-to-end request flows** across services
* Visualize **latency bottlenecks**
* Break down execution into **segments and subsegments**
* Show **service maps** and **dependency graphs**

#### ‚úÖ Best For:

* **Microservices debugging** (e.g., what failed in a request chain)
* **Performance bottleneck tracing** (slow database calls, downstream latency)
* **Tracing Lambda ‚Üí SQS ‚Üí API Gateway ‚Üí ECS**, etc.
* Viewing **per-request context**

---

### üîç **How They Differ**

| Feature              | CloudWatch                   | X-Ray                        |
| -------------------- | ---------------------------- | ---------------------------- |
| **Type**             | Logs + Metrics               | Request tracing              |
| **Granularity**      | Aggregated metrics, raw logs | Per-request flow             |
| **Focus**            | System-level monitoring      | Application-level debugging  |
| **View**             | Time series, dashboards      | Trace maps, waterfall graphs |
| **Latency analysis** | Basic via metrics            | Detailed subsegment timing   |
| **Cost impact**      | Log volume-based             | Trace segment count-based    |

---

### üõ† **Use Both Together (Best Practice)**

For **microservices in EKS / Lambda / ECS**, using **CloudWatch + X-Ray together** gives full visibility:

#### Example Use Case:

üì¶ *Microservice A* calls *Service B*, which calls *Service C*. You're seeing errors and latency.

* **CloudWatch Logs**: Show ERRORs in *Service A*, but not why.
* **CloudWatch Metrics**: Show elevated latency in *Service A*.
* **X-Ray Trace**: Shows that *Service B* is timing out on *Service C* call.

> üéØ **X-Ray provides the root cause**, CloudWatch shows the **symptoms**.

---

### üß† **When to Use What**

| Scenario                          | Use CloudWatch | Use X-Ray                    |
| --------------------------------- | -------------- | ---------------------------- |
| CPU, memory, disk monitoring      | ‚úÖ              | ‚ùå                            |
| Request latency breakdown         | ‚ö†Ô∏è Limited     | ‚úÖ                            |
| Alert when API 5xx > 1%           | ‚úÖ (Alarms)     | ‚ùå                            |
| Trace "which service is slow"     | ‚ùå              | ‚úÖ                            |
| Debug a single failed request     | ‚ùå              | ‚úÖ                            |
| Monitor logs for ‚Äúerror‚Äù patterns | ‚úÖ              | ‚ùå                            |
| Create dashboards across services | ‚úÖ              | ‚ö†Ô∏è (Only basic map in X-Ray) |

---

### üîÑ **Integration Between the Two**

* **X-Ray Traces in CloudWatch ServiceLens**: CloudWatch integrates with X-Ray to provide:

  * **Service Maps**
  * **Trace summaries**
  * **Metric overlays with trace timelines**

* **CloudWatch Logs Correlation**:

  * Inject **trace IDs** into logs so that you can cross-reference logs and traces.
  * For example, in Lambda:

    ```python
    import aws_xray_sdk.core as xray
    xray_recorder = xray.xray_recorder
    trace_id = xray_recorder.current_segment().trace_id
    ```

---

### üîß Example Architecture:

```
             +-------------+
             |  CloudWatch |
             | (metrics)   |
             +------+------+
                    |
+-------------+     |         +----------+
| Lambda/API  +-----+-------->+  X-Ray   |
| Logs/Errors |               | (traces) |
+------+------+\              +----------+
       |       \ Logs
       |        \
+------+---------v--+
| CloudWatch Logs   |
+-------------------+
```

---

### üîö Summary

* **CloudWatch** = Monitoring + Logging
* **X-Ray** = Distributed request tracing
* **Use both together** in microservices for root cause analysis, alerting, and performance visibility.

---

## üõ°Ô∏è **16. Security & IAM Permissions for CloudWatch**

Security in Amazon CloudWatch revolves around **fine-grained IAM permissions**, **cross-account access**, and **least-privilege principles** for logs, metrics, alarms, and dashboards.

---

### üîë **1. Least-Privilege Access for CloudWatch Components**

Use IAM to grant only the minimum required actions:

#### ‚úÖ CloudWatch Metrics (Read-only example)

```json
{
  "Effect": "Allow",
  "Action": [
    "cloudwatch:GetMetricData",
    "cloudwatch:ListMetrics",
    "cloudwatch:GetMetricStatistics"
  ],
  "Resource": "*"
}
```

#### ‚úÖ CloudWatch Logs (Write example for applications)

```json
{
  "Effect": "Allow",
  "Action": [
    "logs:CreateLogGroup",
    "logs:CreateLogStream",
    "logs:PutLogEvents"
  ],
  "Resource": [
    "arn:aws:logs:region:account-id:log-group:/aws/lambda/*"
  ]
}
```

#### ‚úÖ CloudWatch Alarms (for automation tools)

```json
{
  "Effect": "Allow",
  "Action": [
    "cloudwatch:PutMetricAlarm",
    "cloudwatch:DescribeAlarms",
    "cloudwatch:DeleteAlarms"
  ],
  "Resource": "*"
}
```

#### ‚úÖ CloudWatch Dashboards

```json
{
  "Effect": "Allow",
  "Action": [
    "cloudwatch:GetDashboard",
    "cloudwatch:PutDashboard",
    "cloudwatch:DeleteDashboards"
  ],
  "Resource": "*"
}
```

---

### üß© **2. Important IAM Actions by Use Case**

| Use Case                               | Required Actions                                           |
| -------------------------------------- | ---------------------------------------------------------- |
| **EC2/Lambda writing logs**            | `logs:PutLogEvents`, `logs:CreateLogStream`                |
| **Read metrics (Grafana, custom app)** | `cloudwatch:GetMetricData`, `cloudwatch:ListMetrics`       |
| **Set up alarms**                      | `cloudwatch:PutMetricAlarm`, `cloudwatch:DescribeAlarms`   |
| **Log retention config**               | `logs:PutRetentionPolicy`                                  |
| **Enable log subscriptions**           | `logs:PutSubscriptionFilter`                               |
| **View dashboards**                    | `cloudwatch:GetDashboard`                                  |
| **Create ServiceLens view**            | Also needs `xray:BatchGetTraces`, `xray:GetTraceSummaries` |

---

### üß∞ **3. IAM Role Example: Lambda Sending Logs**

This is typically added via **execution role**:

```json
{
  "Effect": "Allow",
  "Action": [
    "logs:CreateLogGroup",
    "logs:CreateLogStream",
    "logs:PutLogEvents"
  ],
  "Resource": "arn:aws:logs:*:*:*"
}
```

---

### üîÑ **4. Cross-Account Monitoring (CloudWatch + IAM Role)**

To centralize monitoring from multiple accounts into one **monitoring account**, use **IAM roles** and **cross-account metric streams**.

#### üîπ Steps:

1. **In Source Account** (App Account):

   * Create IAM Role with `cloudwatch:*` access and `trust policy` allowing the monitoring account.

   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::<monitoring-account-id>:root"
         },
         "Action": "sts:AssumeRole"
       }
     ]
   }
   ```

2. **In Monitoring Account**:

   * AssumeRole and call `GetMetricData`, `DescribeAlarms`, etc.

   ```bash
   aws sts assume-role \
     --role-arn arn:aws:iam::<app-account>:role/CWReadOnlyRole \
     --role-session-name CloudWatchCrossAccount
   ```

3. Or use **CloudWatch cross-account dashboards/metrics stream** feature.

---

### üõéÔ∏è **5. Tips for IAM Best Practices**

‚úÖ **Apply least privilege always** ‚Äì never use `*` in actions unless necessary.
‚úÖ Use **resource-level scoping** (`logs:log-group:/aws/lambda/my-func`)
‚úÖ Use **AWS managed policies** for common use cases (e.g. `CloudWatchReadOnlyAccess`)
‚úÖ Use **permissions boundaries** and **conditions** (e.g. limit region, VPC)
‚úÖ Rotate and audit IAM roles regularly
‚úÖ Enable **CloudTrail** to monitor CloudWatch access

---

### üîê **Security Audit Tip**

Use:

```bash
aws iam simulate-principal-policy \
  --policy-source-arn arn:aws:iam::123456789012:user/my-user \
  --action-names cloudwatch:GetMetricData
```

‚Ä¶to validate permissions **before granting**.

---

### üß† Summary

* Grant **logs\:PutLogEvents** for writers, **GetMetricData** for readers.
* Use **resource-level policies** for tighter control.
* For **cross-account**, use trusted roles or metric streams.
* Monitor access with **CloudTrail** and audit with IAM Access Analyzer.

---


## üí∏ **17. Billing & Cost Optimization in CloudWatch**

CloudWatch offers powerful observability, but **custom metrics**, **log storage**, and **dashboard queries** can get expensive if unmanaged. Here‚Äôs how to estimate, monitor, and optimize CloudWatch usage costs.

---

### üßæ **1. Cost Components in CloudWatch**

| Feature                                      | Pricing (approx.)                                                 |
| -------------------------------------------- | ----------------------------------------------------------------- |
| **Custom Metrics**                           | \$0.30 per metric per month (standard resolution)                 |
| **High-Resolution Metrics**                  | \$0.30 per metric per month (1-second resolution)                 |
| **Logs Ingestion**                           | \$0.50 per GB                                                     |
| **Logs Archival (retention beyond 30 days)** | \$0.03 per GB per month                                           |
| **Dashboards**                               | First 3 free, then \$3 per dashboard/month                        |
| **Alarms**                                   | \$0.10/month per standard alarm, \$0.30/month per composite alarm |
| **Anomaly Detection**                        | \$0.30/month per metric with anomaly detection                    |

> ‚ö†Ô∏è *Free tier*: 10 custom metrics, 5GB logs ingestion, 3 dashboards (for 1 month).

---

### üìä **2. Cost Estimation Examples**

#### üìå **Log Ingestion Cost**

To estimate:

* Each log event is roughly \~100 bytes to 1 KB
* 1 million log lines/day = \~1 GB/day

**Ingestion Cost =**
`(Log Volume in GB) √ó $0.50`

**Retention Cost =**
`(Archived GB √ó $0.03) √ó Retention months`

> E.g., 30 GB/day √ó 30 days = 900 GB ‚Üí \$450/month for ingestion
> Archival beyond 30 days = 900 GB √ó \$0.03 = **\$27/month**

---

### üß† **3. Optimizing Costs**

#### ‚úÖ **Custom Metrics**

* Prefer **embedded metrics format (EMF)** to combine multiple dimensions in one metric
* Remove unused metrics (e.g., from Lambda)

#### ‚úÖ **Logs**

* Set **shorter retention policies**:

  ```bash
  aws logs put-retention-policy \
    --log-group-name /aws/lambda/my-service \
    --retention-in-days 7
  ```
* Route logs to **S3** for long-term retention ‚Üí analyze with **Athena**

#### ‚úÖ **Dashboards**

* Limit to **3 free dashboards** where possible
* Reuse widgets across multiple services

#### ‚úÖ **Alarms**

* **Use composite alarms**: Combine multiple alerts into one logical unit to reduce noise and billing
* Enable **anomaly detection** only where necessary (each costs \$0.30/mo)

---

### üö® **4. Use Composite Alarms to Cut Alert Noise & Cost**

**Example**: Combine 5 microservice alarms into one composite:

```yaml
AlarmName: HighErrorRateComposite
AlarmRule: "ALARM(Service1HighErrorRate) OR ALARM(Service2HighErrorRate)"
```

üîπ **Benefit**: You still get notified, but only one alarm triggers, reducing noise and billing (you pay for just the composite).

---

### üìâ **5. Use CloudWatch Anomaly Detection Wisely**

Anomaly detection costs **\$0.30 per metric/month**, so:

* Use only for **high-priority metrics**
* Pair with **composite alarms** to avoid spamming notifications
* Avoid enabling it on **every dimension (e.g., per AZ, per instance)**

---

### üõ†Ô∏è **6. Monitor & Audit CloudWatch Usage**

Use AWS tools:

* **Cost Explorer** ‚Üí Filter by ‚ÄúCloudWatch‚Äù
* **AWS Budgets** ‚Üí Set budget for CloudWatch spend
* **CloudWatch Usage Metrics**:

  * `AWS/Usage` namespace
  * Example metric: `ResourceCount` with `Type=CloudWatch-Logs`

---

### üí° **Tips**

| Goal                | Strategy                                                        |
| ------------------- | --------------------------------------------------------------- |
| Reduce log costs    | Lower retention, filter before writing, use Firehose ‚Üí S3       |
| Reduce alarm cost   | Use composite alarms and group services                         |
| Reduce metric costs | Collapse related metrics via EMF or CloudWatch Embedded         |
| Long-term analytics | Export to S3 + query via Athena instead of retaining in CW Logs |

---

### ‚úÖ Summary

* **Ingestion** and **custom metrics** are the biggest cost drivers
* Set **retention wisely** and **offload to S3** when needed
* **Composite alarms** and **anomaly detection** reduce alert fatigue and cost
* Monitor CW usage in **Cost Explorer + CW metrics**

